"""
Machine Learning Models for Humidity Prediction in Senegal
Version finale avec sauvegarde du meilleur mod√®le
"""

# ==================== IMPORTS ====================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import pickle
import joblib
import os

from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor

import xgboost as xgb

import warnings
warnings.filterwarnings('ignore')

print("‚úÖ Toutes les biblioth√®ques import√©es avec succ√®s!")

# ==================== LOADING DATA ====================
print("\n" + "="*50)
print("CHARGEMENT DES DONN√âES")
print("="*50)

df = pd.read_csv('meteo_departements_Senegal.csv')
print(f"‚úÖ Donn√©es charg√©es: {df.shape[0]} lignes, {df.shape[1]} colonnes")

# ==================== DATA CLEANING ====================
print("\n" + "="*50)
print("NETTOYAGE DES DONN√âES")
print("="*50)

# Supprimer les doublons
df_clean = df.drop_duplicates().copy()
print(f"‚úÖ Doublons supprim√©s. Donn√©es apr√®s nettoyage: {df_clean.shape[0]} lignes")

# Convertir la colonne date
df_clean['date'] = pd.to_datetime(df_clean['date'])

# Extraire des caract√©ristiques de la date
df_clean['mois'] = df_clean['date'].dt.month
df_clean['jour'] = df_clean['date'].dt.day
df_clean['heure'] = df_clean['date'].dt.hour

print(f"‚úÖ Caract√©ristiques temporelles extraites")

# ==================== FEATURE PREPARATION ====================
print("\n" + "="*50)
print("PR√âPARATION DES CARACT√âRISTIQUES")
print("="*50)

# Encoder les cat√©gories
le_region_dict = {val: idx for idx, val in enumerate(df_clean['region'].unique())}
le_departement_dict = {val: idx for idx, val in enumerate(df_clean['departement'].unique())}

# Encoder weather avec un ordre logique
weather_order = {
    'clear sky': 0,
    'few clouds': 1,
    'scattered clouds': 2,
    'broken clouds': 3,
    'overcast clouds': 4,
    'light rain': 5,
    'moderate rain': 6,
    'heavy rain': 7,
    'thunderstorm with rain': 8
}

def encode_weather(weather):
    return weather_order.get(weather, 4)

# Appliquer l'encodage
df_encoded = df_clean.copy()
df_encoded['region_code'] = df_encoded['region'].map(le_region_dict)
df_encoded['departement_code'] = df_encoded['departement'].map(le_departement_dict)
df_encoded['weather_code'] = df_encoded['weather'].apply(encode_weather)

# S√©lectionner les features finales
feature_columns = [
    'region_code',
    'departement_code',
    'weather_code',
    'temperature',
    'wind_speed',
    'mois',
    'jour',
    'heure'
]

X = df_encoded[feature_columns]
y = df_encoded['humidity']

print(f"‚úÖ Features s√©lectionn√©es: {len(feature_columns)}")

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print(f"\n‚úÖ Split train/test (80/20):")
print(f"   - Training: {X_train.shape[0]} √©chantillons")
print(f"   - Test: {X_test.shape[0]} √©chantillons")

# Standardisation
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print(f"‚úÖ Donn√©es standardis√©es")

# ==================== MODEL TRAINING ====================
print("\n" + "="*50)
print("ENTRA√éNEMENT DES MOD√àLES")
print("="*50)

models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'ElasticNet': ElasticNet(),
    'Random Forest': RandomForestRegressor(random_state=42, n_jobs=-1),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'XGBoost': xgb.XGBRegressor(random_state=42, n_jobs=-1)
}

results = []

for name, model in models.items():
    print(f"\nüîß Entra√Ænement: {name}...")
    
    if name in ['Linear Regression', 'Ridge', 'Lasso', 'ElasticNet']:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
    
    mse = mean_squared_error(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mse)
    
    results.append({
        'Model': name,
        'RMSE': rmse,
        'MAE': mae,
        'R¬≤': r2,
        'MSE': mse
    })
    
    print(f"   - RMSE: {rmse:.2f}, MAE: {mae:.2f}, R¬≤: {r2:.4f}")

# ==================== RESULTS COMPARISON ====================
print("\n" + "="*50)
print("COMPARAISON DES MOD√àLES")
print("="*50)

results_df = pd.DataFrame(results)
results_df = results_df.sort_values('RMSE')
print("\nüìä R√©sultats par RMSE (du meilleur au pire):")
print(results_df.to_string(index=False))

# ==================== SELECT BEST MODEL ====================
print("\n" + "="*50)
print("S√âLECTION DU MEILLEUR MOD√àLE")
print("="*50)

# Entra√Æner √† nouveau le meilleur mod√®le sur toutes les donn√©es
best_model_name = results_df.iloc[0]['Model']
print(f"üèÜ Meilleur mod√®le: {best_model_name}")

# Recr√©er le meilleur mod√®le
if best_model_name in ['Linear Regression', 'Ridge', 'Lasso', 'ElasticNet']:
    best_model = models[best_model_name]
    best_model.fit(X_train_scaled, y_train)
    use_scaler = True
else:
    best_model = models[best_model_name]
    best_model.fit(X_train, y_train)
    use_scaler = False

print(f"‚úÖ Mod√®le r√©entra√Æn√© sur l'ensemble des donn√©es d'entra√Ænement")

# √âvaluation finale
if use_scaler:
    y_pred = best_model.predict(X_test_scaled)
else:
    y_pred = best_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"\nüìä Performance finale:")
print(f"   - RMSE: {rmse:.2f}")
print(f"   - MAE: {mae:.2f}")
print(f"   - R¬≤: {r2:.4f}")

# Feature importance
if hasattr(best_model, 'feature_importances_'):
    print("\nüìä Importance des caract√©ristiques:")
    importance_df = pd.DataFrame({
        'feature': feature_columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)
    print(importance_df.to_string(index=False))

# ==================== SAVE MODEL ====================
print("\n" + "="*50)
print("SAUVEGARDE DU MOD√àLE")
print("="*50)

# Cr√©er le dossier models s'il n'existe pas
os.makedirs('models', exist_ok=True)

# Sauvegarder le mod√®le
model_filename = 'models/best_humidity_model.pkl'
joblib.dump(best_model, model_filename)
print(f"‚úÖ Mod√®le sauvegard√©: {model_filename}")

# Sauvegarder le scaler
scaler_filename = 'models/scaler.pkl'
joblib.dump(scaler, scaler_filename)
print(f"‚úÖ Scaler sauvegard√©: {scaler_filename}")

# Sauvegarder les encoders
encoders = {
    'region_dict': le_region_dict,
    'departement_dict': le_departement_dict,
    'weather_order': weather_order
}
encoders_filename = 'models/encoders.pkl'
joblib.dump(encoders, encoders_filename)
print(f"‚úÖ Encoders sauvegard√©s: {encoders_filename}")

# Sauvegarder les feature columns
feature_columns_filename = 'models/feature_columns.pkl'
joblib.dump(feature_columns, feature_columns_filename)
print(f"‚úÖ Feature columns sauvegard√©es: {feature_columns_filename}")

# Sauvegarder les m√©tadonn√©es du mod√®le
model_metadata = {
    'model_name': best_model_name,
    'use_scaler': use_scaler,
    'feature_columns': feature_columns,
    'train_rmse': rmse,
    'train_mae': mae,
    'train_r2': r2,
    'training_date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
    'n_samples_train': len(X_train),
    'n_samples_test': len(X_test)
}
metadata_filename = 'models/model_metadata.pkl'
joblib.dump(model_metadata, metadata_filename)
print(f"‚úÖ M√©tadonn√©es sauvegard√©es: {metadata_filename}")

# Cr√©er un fichier de sauvegarde avec toutes les informations
summary_filename = 'models/model_summary.txt'
with open(summary_filename, 'w', encoding='utf-8') as f:
    f.write("="*50 + "\n")
    f.write("R√âSUM√â DU MEILLEUR MOD√àLE\n")
    f.write("="*50 + "\n\n")
    f.write(f"üìÖ Date d'entra√Ænement: {model_metadata['training_date']}\n")
    f.write(f"üèÜ Mod√®le: {best_model_name}\n")
    f.write(f"üìä Performance:\n")
    f.write(f"   - RMSE: {rmse:.2f}\n")
    f.write(f"   - MAE: {mae:.2f}\n")
    f.write(f"   - R¬≤: {r2:.4f}\n\n")
    f.write(f"üìà Donn√©es:\n")
    f.write(f"   - √âchantillons d'entra√Ænement: {model_metadata['n_samples_train']}\n")
    f.write(f"   - √âchantillons de test: {model_metadata['n_samples_test']}\n\n")
    f.write(f"üîß Utilise le scaler: {use_scaler}\n\n")
    f.write(f"üìã Caract√©ristiques:\n")
    for feat in feature_columns:
        f.write(f"   - {feat}\n")
    f.write("\n" + "="*50 + "\n")
    f.write("Comparaison avec autres mod√®les:\n")
    f.write("="*50 + "\n")
    f.write(results_df.to_string())

print(f"‚úÖ R√©sum√© sauvegard√©: {summary_filename}")

# ==================== VISUALIZATIONS ====================
print("\n" + "="*50)
print("G√âN√âRATION DES VISUALISATIONS")
print("="*50)

# Cr√©er le dossier images s'il n'existe pas
os.makedirs('images', exist_ok=True)

# 1. Comparaison des mod√®les
plt.figure(figsize=(10, 6))
results_df_top = results_df.head(5)
plt.barh(results_df_top['Model'], results_df_top['RMSE'])
plt.xlabel('RMSE (Root Mean Squared Error)')
plt.title('Comparaison des 5 meilleurs mod√®les')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('images/model_comparison.png', dpi=150, bbox_inches='tight')
print("‚úÖ Graphique de comparaison sauv√©: images/model_comparison.png")

# 2. Pr√©dictions vs R√©alit√©
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Valeurs r√©elles')
plt.ylabel('Pr√©dictions')
plt.title(f'Pr√©dictions vs R√©alit√© - {best_model_name}')
plt.tight_layout()
plt.savefig('images/predictions_vs_reality.png', dpi=150, bbox_inches='tight')
print("‚úÖ Graphique pr√©dictions/r√©alit√© sauv√©: images/predictions_vs_reality.png")

# 3. Feature importance
if hasattr(best_model, 'feature_importances_'):
    plt.figure(figsize=(10, 6))
    importance_df.plot(x='feature', y='importance', kind='barh', legend=False)
    plt.xlabel('Importance')
    plt.title(f'Importance des caract√©ristiques - {best_model_name}')
    plt.tight_layout()
    plt.savefig('images/feature_importance.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Graphique d'importance sauv√©: images/feature_importance.png")

print("\n" + "="*50)
print("‚úÖ PROCESSUS TERMIN√â AVEC SUCC√àS!")
print("="*50)
print(f"\nüèÜ Fichiers cr√©√©s dans le dossier 'models/':")
print(f"   - best_humidity_model.pkl (mod√®le)")
print(f"   - scaler.pkl (standardisation)")
print(f"   - encoders.pkl (encodages)")
print(f"   - feature_columns.pkl (colonnes)")
print(f"   - model_metadata.pkl (m√©tadonn√©es)")
print(f"   - model_summary.txt (r√©sum√© lisible)")
print(f"\nüìä Graphiques dans 'images/':")
print(f"   - model_comparison.png")
print(f"   - predictions_vs_reality.png")
print(f"   - feature_importance.png")

